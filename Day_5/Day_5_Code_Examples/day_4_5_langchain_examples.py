# -*- coding: utf-8 -*-
"""Day_4_LangChain_Examples.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yCNyjdBf_Gdt3m3BLSz6ID2XlvuCRuSm
"""

# Install necessary libraries
!pip install -q langchain openai
!pip install -q langchain openai langchain_openai

# Import necessary modules
import os
from langchain.prompts import PromptTemplate
from langchain_openai import OpenAI
from google.colab import userdata

# Set your OpenAI API key
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')

# Define the prompt template
template = """
You are a helpful assistant that answers questions about {topic}.

Question: {question}
Answer:
"""

prompt = PromptTemplate(
    input_variables=["topic", "question"],
    template=template,
)

# Initialize the OpenAI model
llm = OpenAI(temperature=0)

# Format the prompt with specific values

formatted_prompt = prompt.format(topic="Cricket", question="Who is Ravi Shastri? and What roles in worked in?")


# Run the prompt through the model
response = llm.invoke(formatted_prompt)

# Print the response
print(response)

topic = input("Enter the topic: ")
question = input("Enter the question: ")

formatted_prompt = prompt.format(topic=topic, question=question)

response = llm.invoke(formatted_prompt)

print(response)

# Chat Template Example Below



from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from google.colab import userdata
import os

# Set your OpenAI API key
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')

# Initialize the OpenAI model
llm = ChatOpenAI(temperature=0)


# âœ… Template with {topic} and {question}
chat_template = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant that answers questions about {topic}."),
        ("human", "{question}")
    ]
)


topic = input("Enter the topic: ")
question = input("Enter the question: ")

formatted_chat_prompt = chat_template.format_messages(topic=topic, question=question)

chat_response = llm.invoke(formatted_chat_prompt)

# Access the content attribute to get the response text
print(chat_response.content)

from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from google.colab import userdata
import os

# Set your OpenAI API key
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')

# Initialize the OpenAI model
llm = ChatOpenAI(temperature=0)


# âœ… Template with {topic} and {question}
chat_template = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant that answers questions about {topic}."),
        ("human", "{question}")
    ]
)


topic = input("Enter the topic: ")
question = input("Enter the question: ")

formatted_chat_prompt = chat_template.format_messages(topic=topic, question=question)

chat_response = llm.invoke(formatted_chat_prompt)

# Access the content attribute to get the response text
print(chat_response.content)

from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from google.colab import userdata
import os

# Set your OpenAI API key
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')

# Initialize the ChatOpenAI model
llm = ChatOpenAI(temperature=0)

# Ask for the topic once
topic = input("Enter the topic: ")

# Loop to ask multiple questions
while True:
    question = input("\nEnter your question (or type 'exit' to quit): ")

    if question.lower() in ['exit', 'quit']:
        print("Exiting... Have a great day!")
        break

    # Format and send the prompt
    chat_template = ChatPromptTemplate.from_messages(
        [
            ("system", "You are a helpful assistant that answers questions about {topic}."),
            ("human", "{question}")
        ]
    )

    formatted_chat_prompt = chat_template.format_messages(topic=topic, question=question)
    chat_response = llm.invoke(formatted_chat_prompt)

    # Show the answer
    print("\nAnswer:", chat_response.content)

# multiple topics

from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from google.colab import userdata
import os

# Set your OpenAI API key
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')

# Initialize the ChatOpenAI model
llm = ChatOpenAI(temperature=0)

print("Welcome! You can ask questions on any topic.")
print("Type 'change topic' to switch topics or 'exit' to quit.\n")

# Master loop for multiple topics
run = True
while run:
    topic = input("Enter the topic you'd like to ask about: ")
    if topic.lower() in ['exit', 'quit']:
        print("Goodbye!")
        break

    # Inner loop for multiple questions under the same topic
    while True:
        question = input("\nAsk your question (or type 'change topic' or 'exit'): ").strip().lower()

        if question == 'exit' or question == 'quit':
            print("Goodbye!")
            run = False  # safely exit both loops
            break
        elif question == 'change topic':
            break

        # Chat prompt template
        chat_template = ChatPromptTemplate.from_messages(
            [
                ("system", "You are a helpful assistant that answers questions about {topic}."),
                ("human", "{question}")
            ]
        )

        formatted_chat_prompt = chat_template.format_messages(topic=topic, question=question)
        chat_response = llm.invoke(formatted_chat_prompt)

        print("\nAnswer:", chat_response.content)

# Chains
 # Example #1 Below

from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from google.colab import userdata
import os

# Set your API key as before
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')

# Define the chat prompt template (system + human)
prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant that answers questions about {topic}."),
        ("human", "{question}")
    ]
)

# Initialize the LLM
llm = ChatOpenAI(temperature=0)

# Chain them together using the Runnable operator
chain = prompt | llm

print("Welcome! You can ask questions on any topic.")
print("Type 'change topic' to switch topics or 'exit' to quit.\n")

run = True
while run:
    topic = input("Enter the topic you'd like to ask about: ")
    if topic.lower() in ['exit', 'quit']:
        print("Goodbye!")
        break

    while True:
        question = input("\nAsk your question (or type 'change topic' or 'exit'): ").strip()
        if question.lower() in ['exit', 'quit']:
            print("Goodbye!")
            run = False
            break
        elif question.lower() == 'change topic':
            break

        # Use the chain for inference (with dictionary input)
        response = chain.invoke({
            "topic": topic,
            "question": question
        })

        print("\nAnswer:", response.content)

# Example #2

from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from google.colab import userdata
import os

# API key setup (same as before)
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')

# A helper function to build conversation history text from memory
def format_history(history):
    # history is a list of (question, answer) tuples
    if not history:
        return "No previous Q&A."
    formatted = ""
    for i, (q, a) in enumerate(history[-5:], start=1):  # last 5 pairs
        formatted += f"Q{i}: {q}\nA{i}: {a}\n"
    return formatted.strip()

# ChatPromptTemplate extended to include conversation history
prompt = ChatPromptTemplate.from_messages(
    [
        ("system",
         "You are a helpful assistant that answers questions about {topic}.\n"
         "Here is the recent conversation history:\n"
         "{history}\n"
         "Answer the next question carefully."),
        ("human", "{question}")
    ]
)

llm = ChatOpenAI(temperature=0)
chain = prompt | llm

print("Welcome! You can ask questions on any topic.")
print("Type 'change topic' to switch topics or 'exit' to quit.\n")

run = True
while run:
    topic = input("Enter the topic you'd like to ask about: ")
    if topic.lower() in ['exit', 'quit']:
        print("Goodbye!")
        break

    conversation_history = []  # to store Q&A pairs for current topic

    while True:
        question = input("\nAsk your question (or type 'change topic' or 'exit'): ").strip()
        if question.lower() in ['exit', 'quit']:
            print("Goodbye!")
            run = False
            break
        elif question.lower() == 'change topic':
            break

        # Generate the formatted history text
        history_text = format_history(conversation_history)

        # Invoke chain, passing topic, question & history
        response = chain.invoke({
            "topic": topic,
            "history": history_text,
            "question": question
        })

        answer = response.content
        print("\nAnswer:", answer)

        # Save the Q&A pair to memory
        conversation_history.append((question, answer))

# Example #3 Chain + Memory

from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from google.colab import userdata
import os
import json

# API key setup
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')

# Helper functions to save/load memory

def history_filename(topic):
    # Sanitizing topic string for filename
    safe_topic = topic.replace(" ", "_").lower()
    return f"memory_{safe_topic}.json"

def load_memory(topic):
    filename = history_filename(topic)
    if os.path.exists(filename):
        with open(filename, 'r') as f:
            return json.load(f)  # expecting list of [question, answer] pairs
    else:
        return []

def save_memory(topic, conversation_history):
    filename = history_filename(topic)
    with open(filename, 'w') as f:
        json.dump(conversation_history, f, indent=2)

def format_history(history):
    if not history:
        return "No previous Q&A."
    formatted = ""
    for i, (q, a) in enumerate(history[-5:], start=1):  # last 5 pairs
        formatted += f"Q{i}: {q}\nA{i}: {a}\n"
    return formatted.strip()

# ChatPromptTemplate including dynamic conversation history
prompt = ChatPromptTemplate.from_messages(
    [
        ("system",
         "You are a helpful assistant that answers questions about {topic}.\n"
         "Here is the recent conversation history:\n"
         "{history}\n"
         "Answer the next question carefully."),
        ("human", "{question}")
    ]
)

llm = ChatOpenAI(temperature=0)
chain = prompt | llm

print("Welcome! You can ask questions on any topic.")
print("Type 'change topic' to switch topics or 'exit' to quit.\n")

run = True
while run:
    topic = input("Enter the topic you'd like to ask about: ").strip()
    if topic.lower() in ['exit', 'quit']:
        print("Goodbye!")
        break

    # Load persistent conversation history for this topic
    conversation_history = load_memory(topic)

    while True:
        question = input("\nAsk your question (or type 'change topic' or 'exit'): ").strip()
        if question.lower() in ['exit', 'quit']:
            print("Goodbye!")
            run = False
            break
        elif question.lower() == 'change topic':
            # Before switching, save current topic's conversation history
            save_memory(topic, conversation_history)
            break

        history_text = format_history(conversation_history)

        response = chain.invoke({
            "topic": topic,
            "history": history_text,
            "question": question
        })

        answer = response.content
        print("\nAnswer:", answer)

        # Save Q&A pair to memory and persist immediately
        conversation_history.append([question, answer])
        save_memory(topic, conversation_history)

# Example #4 -> with history (MessagesPlaceHolder & Buffer Memory)

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from langchain_core.runnables import Runnable
import os
from google.colab import userdata

# Set API key (adjust if not in Colab)
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')

# --- Helper: in-memory memory store per topic ---
# For persistence across script restarts, you could save/load memory states to disk or DB.
# Here we just keep in RAM for demonstration.

memory_store = {}

def get_memory_for_topic(topic):
    if topic not in memory_store:
        # Initialize a new memory instance for each topic
        memory_store[topic] = ConversationBufferMemory(memory_key="history", return_messages=True)
    return memory_store[topic]

# --- Define prompt with MessagesPlaceholder ---

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful assistant that answers questions about {topic}."),
        MessagesPlaceholder(variable_name="history"),  # Inject chat history here automatically
        ("human", "{question}")
    ]
)

# Initialize LLM
llm = ChatOpenAI(temperature=0)

# Create a chain composed of prompt followed by llm
chain = prompt | llm

print("Welcome! You can ask questions on any topic.")
print("Type 'change topic' to switch topics or 'exit' to quit.\n")

run = True
while run:
    topic = input("Enter the topic you'd like to ask about: ").strip()
    if topic.lower() in ['exit', 'quit']:
        print("Goodbye!")
        break

    # Get (or create) memory for this topic
    memory = get_memory_for_topic(topic)

    # We'll associate the current Topic variable to memory as context below
    while True:
        question = input("\nAsk your question (or type 'change topic' or 'exit'): ").strip()
        if question.lower() in ['exit', 'quit']:
            print("Goodbye!")
            run = False
            break
        if question.lower() == 'change topic':
            print(f"Switching from topic '{topic}'\n")
            break

        # Prepare inputs including memory context
        inputs = {
            "topic": topic,
            "question": question,
            "history": memory.load_memory_variables({}).get("history", [])  # load_messages from memory
        }

        # Invoke chain with inputs; chain sees 'history' as part of MessagesPlaceholder
        response = chain.invoke(inputs)

        # Print the assistant's reply
        print("\nAnswer:", response.content)

        # Save conversation turns to memory
        # Add user's question message
        memory.save_context({"human": question}, {"ai": response.content})

print("\n--- Conversation History ---")
for msg in memory.load_memory_variables({}).get("history", []):
    print(f"[{msg.type}] {msg.content}")
print("----------------------------\n")

# Example #5 - Persistent Memory Management (using Manual Session-ID)

import os
import json
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from google.colab import userdata

# Set your API key (adjust if not in Colab)
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')

# --- Persistent Memory Helpers (per session and topic) ---

def make_memory_filename(session_id: str, topic: str) -> str:
    safe_session = session_id.replace(" ", "_").lower()
    safe_topic = topic.replace(" ", "_").lower()
    return f"memory_{safe_session}_{safe_topic}.json"

def save_memory_to_file(conversation_history, filename):
    # conversation_history: list of messages (dict form)
    # Save messages as list of dicts
    with open(filename, 'w') as f:
        json.dump(conversation_history, f, indent=2)

def load_memory_from_file(filename):
    if os.path.exists(filename):
        with open(filename, 'r') as f:
            return json.load(f)
    return []

def convert_msgs_for_memory(messages_json):
    """Convert loaded JSON into LangChain Message objects"""
    from langchain.schema import HumanMessage, AIMessage, SystemMessage

    msg_list = []
    for msg in messages_json:
        # Each msg is a dict with "type" and "content"
        t = msg.get("type")
        c = msg.get("content")
        if t == "human":
            msg_list.append(HumanMessage(content=c))
        elif t == "ai":
            msg_list.append(AIMessage(content=c))
        elif t == "system":
            msg_list.append(SystemMessage(content=c))
        else:
            # Default fallback
            msg_list.append(HumanMessage(content=c))
    return msg_list

def convert_msgs_to_json(messages):
    """Convert LangChain Message objects to simple JSON serializable dicts"""
    json_list = []
    for msg in messages:
        # msg.type can be 'human', 'ai', 'system'
        json_list.append({
            "type": msg.type,
            "content": msg.content
        })
    return json_list

# --- Session & Topic Memory Management ---

# Maintain in-memory ConversationBufferMemory instances per session & topic
memory_store = {}

def get_memory(session_id, topic):
    key = (session_id, topic)
    if key in memory_store:
        return memory_store[key]

    # Else create new memory and try loading stored history from disk
    memory = ConversationBufferMemory(memory_key="history", return_messages=True)

    # Load history from file if exists
    filename = make_memory_filename(session_id, topic)
    loaded_msgs_json = load_memory_from_file(filename)
    if loaded_msgs_json:
        # Convert JSON back to LangChain Messages and set memory state manually
        from langchain.memory.chat_memory import _messages_from_dict
        # Instead of _messages_from_dict (private), we use convert_msgs_for_memory here
        loaded_msgs = convert_msgs_for_memory(loaded_msgs_json)
        memory.chat_memory.messages = loaded_msgs

    memory_store[key] = memory
    return memory

def save_memory(session_id, topic, memory):
    filename = make_memory_filename(session_id, topic)
    # Extract messages and convert them to JSON serializable form
    messages = memory.load_memory_variables({}).get("history", [])
    msgs_json = convert_msgs_to_json(messages)
    save_memory_to_file(msgs_json, filename)


# --- Prompt and Chain Setup ---

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant that answers questions about {topic}."),
    MessagesPlaceholder(variable_name="history"),  # Automatically insert conversation history
    ("human", "{question}")
])

llm = ChatOpenAI(temperature=0)
chain = prompt | llm

# --- Main conversational loop with sessions and topics ---

print("Welcome! This chat supports multiple sessions & topics with persistent memory.")
print("Type 'exit' to quit at any prompt.")
print("Type 'change topic' during questioning to pick a new topic.")
print("Type 'show history' during questioning to display conversation history.\n")

run = True
while run:
    session_id = input("Enter your session id (e.g. user name or unique ID): ").strip()
    if session_id.lower() in ['exit', 'quit']:
        print("Goodbye!")
        break

    while True:
        topic = input(f"Enter the topic for session '{session_id}': ").strip()
        if topic.lower() in ['exit', 'quit']:
            print("Goodbye!")
            run = False
            break
        if not topic:
            print("Please enter a valid topic.")
            continue

        # Get memory object for session + topic
        memory = get_memory(session_id, topic)

        # Conversation loop per topic & session
        while True:
            question = input(f"\n[{session_id} | {topic}] Ask a question ('change topic', 'show history', 'exit'): ").strip()
            lower_q = question.lower()

            if lower_q in ['exit', 'quit']:
                # Save memory and exit everything
                save_memory(session_id, topic, memory)
                print("Goodbye!")
                run = False
                break
            elif lower_q == 'change topic':
                # Save memory and break to outer topic selection
                save_memory(session_id, topic, memory)
                print(f"Changing topic from '{topic}'\n")
                break
            elif lower_q == 'show history':
                # Show current conversation history nicely
                msgs = memory.load_memory_variables({}).get("history", [])
                if not msgs:
                    print("\nNo conversation history yet.")
                else:
                    print("\n--- Conversation History ---")
                    for i, msg in enumerate(msgs, start=1):
                        role = "User" if msg.type == "human" else "Assistant" if msg.type == "ai" else "System"
                        print(f"{i}. {role}: {msg.content}")
                    print("----------------------------\n")
                continue

            # Run chain with current inputs + memory
            inputs = {
                "topic": topic,
                "question": question,
                "history": memory.load_memory_variables({}).get("history", [])
            }

            response = chain.invoke(inputs)
            answer = response.content
            print("\nAnswer:", answer)

            # Save context to memory and persist state
            memory.save_context({"human": question}, {"ai": answer})
            save_memory(session_id, topic, memory)

# Example #6 - Persistent Memory Management (using System Generated Session-ID)

import os
import uuid
import json
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from google.colab import userdata  # Remove if not running in Colab

# Set your OpenAI API key
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')

# --- Persistent Memory Helpers (per session and topic) ---

def make_memory_filename(session_id: str, topic: str) -> str:
    safe_session = session_id.replace(" ", "_").lower()
    safe_topic = topic.replace(" ", "_").lower()
    return f"memory_{safe_session}_{safe_topic}.json"

def save_memory_to_file(conversation_history, filename):
    with open(filename, 'w') as f:
        json.dump(conversation_history, f, indent=2)

def load_memory_from_file(filename):
    if os.path.exists(filename):
        with open(filename, 'r') as f:
            return json.load(f)
    return []

def convert_msgs_for_memory(messages_json):
    from langchain.schema import HumanMessage, AIMessage, SystemMessage

    msg_list = []
    for msg in messages_json:
        t = msg.get("type")
        c = msg.get("content")
        if t == "human":
            msg_list.append(HumanMessage(content=c))
        elif t == "ai":
            msg_list.append(AIMessage(content=c))
        elif t == "system":
            msg_list.append(SystemMessage(content=c))
        else:
            msg_list.append(HumanMessage(content=c))
    return msg_list

def convert_msgs_to_json(messages):
    json_list = []
    for msg in messages:
        json_list.append({
            "type": msg.type,
            "content": msg.content
        })
    return json_list

# --- Session & Topic Memory Management ---

memory_store = {}

def get_memory(session_id, topic):
    key = (session_id, topic)
    if key in memory_store:
        return memory_store[key]

    memory = ConversationBufferMemory(memory_key="history", return_messages=True)
    filename = make_memory_filename(session_id, topic)
    loaded_msgs_json = load_memory_from_file(filename)
    if loaded_msgs_json:
        loaded_msgs = convert_msgs_for_memory(loaded_msgs_json)
        memory.chat_memory.messages = loaded_msgs

    memory_store[key] = memory
    return memory

def save_memory(session_id, topic, memory):
    filename = make_memory_filename(session_id, topic)
    messages = memory.load_memory_variables({}).get("history", [])
    msgs_json = convert_msgs_to_json(messages)
    save_memory_to_file(msgs_json, filename)


# --- Prompt and Chain Setup ---

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant that answers questions about {topic}."),
    MessagesPlaceholder(variable_name="history"),  # Inject conversation history
    ("human", "{question}")
])

llm = ChatOpenAI(temperature=0)
chain = prompt | llm


# --- Main conversational loop with session id generation ---

print("Welcome! This chat supports multiple sessions & topics with persistent memory.")
print("Type 'exit' to quit at any prompt.")
print("Type 'change topic' during questioning to pick a new topic.")
print("Type 'show history' during questioning to display conversation history.\n")

run = True
while run:
    session_id = input("Enter your session id (leave blank for auto-generated): ").strip()
    if session_id.lower() in ['exit', 'quit']:
        print("Goodbye!")
        break
    if not session_id:
        session_id = str(uuid.uuid4())
        print(f"Your generated session id: {session_id}")

    while True:
        topic = input(f"Enter the topic for session '{session_id}': ").strip()
        if topic.lower() in ['exit', 'quit']:
            print("Goodbye!")
            run = False
            break
        if not topic:
            print("Please enter a valid topic.")
            continue

        memory = get_memory(session_id, topic)

        while True:
            question = input(f"\n[{session_id} | {topic}] Ask a question ('change topic', 'show history', 'exit'): ").strip()
            lower_q = question.lower()

            if lower_q in ['exit', 'quit']:
                save_memory(session_id, topic, memory)
                print("Goodbye!")
                run = False
                break
            elif lower_q == 'change topic':
                save_memory(session_id, topic, memory)
                print(f"Changing topic from '{topic}'\n")
                break
            elif lower_q == 'show history':
                msgs = memory.load_memory_variables({}).get("history", [])
                if not msgs:
                    print("\nNo conversation history yet.")
                else:
                    print("\n--- Conversation History ---")
                    for i, msg in enumerate(msgs, start=1):
                        role = "User" if msg.type == "human" else "Assistant" if msg.type == "ai" else "System"
                        print(f"{i}. {role}: {msg.content}")
                    print("----------------------------\n")
                continue

            inputs = {
                "topic": topic,
                "question": question,
                "history": memory.load_memory_variables({}).get("history", [])
            }

            response = chain.invoke(inputs)
            answer = response.content
            print("\nAnswer:", answer)

            memory.save_context({"human": question}, {"ai": answer})
            save_memory(session_id, topic, memory)

# Example #7 -> Write history to csv

import os
import uuid
import json
import csv
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain.memory import ConversationBufferMemory
from google.colab import userdata  # Remove or replace if not in Colab

# Set your OpenAI API key from Colab userdata
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')

# --- Persistent Memory Helpers (per session and topic) ---

def make_memory_filename(session_id: str, topic: str) -> str:
    safe_session = session_id.replace(" ", "_").lower()
    safe_topic = topic.replace(" ", "_").lower()
    return f"memory_{safe_session}_{safe_topic}.json"

def save_memory_to_file(conversation_history, filename):
    with open(filename, 'w') as f:
        json.dump(conversation_history, f, indent=2)

def load_memory_from_file(filename):
    if os.path.exists(filename):
        with open(filename, 'r') as f:
            return json.load(f)
    return []

def convert_msgs_for_memory(messages_json):
    from langchain.schema import HumanMessage, AIMessage, SystemMessage

    msg_list = []
    for msg in messages_json:
        t = msg.get("type")
        c = msg.get("content")
        if t == "human":
            msg_list.append(HumanMessage(content=c))
        elif t == "ai":
            msg_list.append(AIMessage(content=c))
        elif t == "system":
            msg_list.append(SystemMessage(content=c))
        else:
            msg_list.append(HumanMessage(content=c))
    return msg_list

def convert_msgs_to_json(messages):
    json_list = []
    for msg in messages:
        json_list.append({
            "type": msg.type,
            "content": msg.content
        })
    return json_list

# --- CSV export helper ---

def save_history_to_csv(history_messages, csv_filename):
    with open(csv_filename, mode='w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(["Index", "Role", "Content"])
        for i, msg in enumerate(history_messages, start=1):
            if msg.type == "human":
                role = "User"
            elif msg.type == "ai":
                role = "Assistant"
            elif msg.type == "system":
                role = "System"
            else:
                role = msg.type.capitalize()
            writer.writerow([i, role, msg.content])
    print(f"Conversation history saved to {csv_filename}")

# --- Session & Topic Memory Management ---

memory_store = {}

def get_memory(session_id, topic):
    key = (session_id, topic)
    if key in memory_store:
        return memory_store[key]

    memory = ConversationBufferMemory(memory_key="history", return_messages=True)
    filename = make_memory_filename(session_id, topic)
    loaded_msgs_json = load_memory_from_file(filename)
    if loaded_msgs_json:
        loaded_msgs = convert_msgs_for_memory(loaded_msgs_json)
        memory.chat_memory.messages = loaded_msgs

    memory_store[key] = memory
    return memory

def save_memory(session_id, topic, memory):
    filename = make_memory_filename(session_id, topic)
    messages = memory.load_memory_variables({}).get("history", [])
    msgs_json = convert_msgs_to_json(messages)
    save_memory_to_file(msgs_json, filename)

# --- Prompt and Chain Setup ---

prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful assistant that answers questions about {topic}."),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{question}")
])

llm = ChatOpenAI(temperature=0)
chain = prompt | llm

# --- Main conversational loop with session ID and commands ---

print("Welcome! This chat supports multiple sessions & topics with persistent memory.")
print("Commands during questioning: 'change topic', 'show history', 'export history', 'exit'.\n")

run = True
while run:
    session_id = input("Enter your session id (leave blank to auto-generate): ").strip()
    if session_id.lower() in ['exit', 'quit']:
        print("Goodbye!")
        break
    if not session_id:
        session_id = str(uuid.uuid4())
        print(f"Your generated session id: {session_id}")

    while True:
        topic = input(f"\nEnter the topic for session '{session_id}': ").strip()
        if topic.lower() in ['exit', 'quit']:
            print("Goodbye!")
            run = False
            break
        if not topic:
            print("Please enter a valid topic.")
            continue

        memory = get_memory(session_id, topic)

        while True:
            question = input(f"\n[{session_id} | {topic}] Ask a question ('change topic', 'show history', 'export history', 'exit'): ").strip()
            lower_q = question.lower()

            if lower_q in ['exit', 'quit']:
                save_memory(session_id, topic, memory)
                print("Goodbye!")
                run = False
                break
            elif lower_q == 'change topic':
                save_memory(session_id, topic, memory)
                print(f"Switching topic from '{topic}'...\n")
                break
            elif lower_q == 'show history':
                msgs = memory.load_memory_variables({}).get("history", [])
                if not msgs:
                    print("\nNo conversation history yet.")
                else:
                    print("\n--- Conversation History ---")
                    for i, msg in enumerate(msgs, start=1):
                        role = "User" if msg.type == "human" else "Assistant" if msg.type == "ai" else "System"
                        print(f"{i}. {role}: {msg.content}")
                    print("----------------------------\n")
                continue
            elif lower_q == 'export history':
                msgs = memory.load_memory_variables({}).get("history", [])
                filename = f"history_{session_id}_{topic}.csv".replace(" ", "_")
                save_history_to_csv(msgs, filename)
                continue

            inputs = {
                "topic": topic,
                "question": question,
                "history": memory.load_memory_variables({}).get("history", [])
            }

            response = chain.invoke(inputs)
            answer = response.content
            print("\nAnswer:", answer)

            memory.save_context({"human": question}, {"ai": answer})
            save_memory(session_id, topic, memory)

# UI - Implementation with Gradio

# UI Implementation

# Chat-bot with Gradio

# Below

# âœ… STEP 1: Install dependencies
!pip install --quiet langchain openai gradio

# âœ… STEP 2: Imports
import gradio as gr
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# âœ… STEP 3: Setup OpenAI key securely
from google.colab import userdata
import os
os.environ["OPENAI_API_KEY"] = userdata.get("OPENAI_API_KEY")

# âœ… STEP 4: Initialize OpenAI LLM
llm = ChatOpenAI(temperature=0)

# âœ… STEP 5: Define the chatbot function
def chatbot_fn(topic, question):
    prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a helpful assistant that answers questions about {topic}."),
        ("human", "{question}")
    ])
    messages = prompt.format_messages(topic=topic, question=question)
    response = llm.invoke(messages)
    return response.content

# âœ… STEP 6: Launch Gradio UI
with gr.Blocks() as demo:
    gr.Markdown("### ðŸ¤– OpenAI Chatbot (LangChain + Gradio in Colab)")
    topic = gr.Textbox(label="Topic", placeholder="e.g. Healthcare, Cricket")
    question = gr.Textbox(label="Your Question", placeholder="Ask your question...")
    response = gr.Textbox(label="Response")
    ask_btn = gr.Button("Ask")
    ask_btn.click(fn=chatbot_fn, inputs=[topic, question], outputs=response)

demo.launch(share=True)  # use share=True to expose public link in Colab

